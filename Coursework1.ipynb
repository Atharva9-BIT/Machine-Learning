{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Importing all the required Libraries. \"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "\"\"\" Importing the data from sklearn instead of txt File. \"\"\"\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Loading the Iris data set into python. \"\"\"\n",
    "iris = load_iris()\n",
    "\"\"\" Looking at the data using description. \"\"\"\n",
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loading Data for ionosphere \"\"\"\n",
    "X = np.genfromtxt(\"ionosphere.txt\", delimiter=\",\", usecols = np.arange(34))   #Storing all Samples \n",
    "\n",
    "y = np.genfromtxt(\"ionosphere.txt\",delimiter=\",\", usecols = 34, dtype= 'int') #storing all labels by just taking last cloumn as array in data y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Splitting and Training the Data into Train and Test set. \"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], random_state=909) #for iris train test split\n",
    "I_X_train, I_X_test, I_y_train, I_y_test = train_test_split(X, y, random_state=909)                 #for ionosphere test split\n",
    "                                                                                                    #random state for getting same ouput for various attempts\n",
    "\n",
    "\n",
    "\"\"\"\"Nearest Neighbors code is written for both datasets below: KNN\"\"\"\"\n",
    "#Expermimenting with KNeighbours for both datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function for Euclidean Distance and Prediction\"\"\"\n",
    "\n",
    "import math\n",
    "def pred(X_train,y_train,X_test):\n",
    "    rows_for_X_test = X_test.shape[0]                 #X_test rows \n",
    "    cols_for_X_test = X_test.shape[1]                 #X_test Cols\n",
    "    rows_for_X_train = X_train.shape[0]               #X_train rows in for loop\n",
    "    cols_for_X_train = X_train.shape[1]               #X_train cols in for loop\n",
    "    arr=[]                                            # Distances of test and sample \n",
    "    test=[]                                           #test values\n",
    "    element=[]                                        #train values\n",
    "    prediction_label=[]                               #storing predicted label\n",
    "\n",
    "    \n",
    "    for n in range(rows_for_X_test):                  #taking X_test rows in for loop\n",
    "        for m in range(cols_for_X_test):              #taking X_test cols in for loop\n",
    "            test.append(X_test[n][m])                 # appending them in test list \n",
    "     \n",
    "\n",
    "        for i in range(rows_for_X_train):             #taking X_train rows in for loop\n",
    "            for j in range(cols_for_X_train):         #taking X_train cols in for loop\n",
    "                element.append(X_train[i][j])         #appending them in element list\n",
    "                \n",
    "            sum_of_points = ((element[0]-test[0])**2+(element[1]-test[1])**2+(element[2]-test[2])**2+(element[2]-test[2])**2)  #sum of points by euclidean distance\n",
    "            d = np.sqrt(sum_of_points)                #square root of distances    \n",
    "            arr.append(d)                             #appending distances in array\n",
    "            del element[:]                            #clearing element array               \n",
    "        min_no = arr.index(min(arr))                  #storing minimum in min_no from arr\n",
    "        prediction_label.append(y_train[min_no])      #predicting the label through min_no\n",
    "        del arr[:]                                    #flushing array of distances \n",
    "        del test[:]\n",
    "    return prediction_label  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Accuracy for prediction Iris\"\"\"\n",
    "Y_pred_Iris = pred(X_train,y_train,X_test)\n",
    "np.mean(Y_pred_Iris == y_test)                        # prediction of y_test samples for Iris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8409090909090909"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Accuracy for prediction for Iono \"\"\"\n",
    "Y_pred_Iono = pred(I_X_train,I_y_train,I_X_test)\n",
    "np.mean(Y_pred_Iono == I_y_test)                      # prediction of y_test sapmles for Ionosphere \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error Rate for Iris:  0.07894736842105265\n",
      " Numbers of errors:  3\n"
     ]
    }
   ],
   "source": [
    "error_rate_iris= (1-np.mean(Y_pred_Iris == y_test))\n",
    "print(\" Error Rate for Iris: \", error_rate_iris)      # Error rate is %%\n",
    "\n",
    "\"\"\" Number of error Rate \"\"\"                            \n",
    "numbers=0\n",
    "array=  y_test.shape[0]                               #storing array \n",
    "for a in range(array):\n",
    "    if(Y_pred_Iris[a] != y_test[a]):                  #comparing elemets which are not equal in Y_pred_iris and y_test and printing them\n",
    "        numbers=numbers+1\n",
    "    a=a+1\n",
    "print(\" Numbers of errors: \",numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error Rate for Iris:  0.15909090909090906\n",
      "Numbers of errors  26\n"
     ]
    }
   ],
   "source": [
    "error_rate_iono = (1-np.mean(Y_pred_Iono == I_y_test))\n",
    "print(\" Error Rate for Iris: \", error_rate_iono)      # Error rate is %%\n",
    "\n",
    "\"\"\" Number of error Rate \"\"\"\n",
    "numbers=0\n",
    "array=y_test.shape[0]                                 #storing array\n",
    "for a in range(array):\n",
    "    if(Y_pred_Iono[a] != y_test[a]):                  #comparing elements which are not equal in Y_pred_iono and y_test and printing them\n",
    "        numbers=numbers+1   \n",
    "print(\"Numbers of errors \",numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.9210526315789473\n",
      " Error Rate for Iris:  0.07894736842105265\n"
     ]
    }
   ],
   "source": [
    "# Using KNN classifier\n",
    "\"\"\"Accuracy for Iris data :\"\"\"\n",
    "knn =  KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "print(\"Accuracy \", knn.score(X_test, y_test))\n",
    "\n",
    "\"\"\" Error Rate for KNN in 3 \"\"\"\n",
    "error_rate_iris= (1-np.mean(Y_pred_Iris == y_test))\n",
    "print(\" Error Rate for Iris: \", error_rate_iris)      # Error rate is %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.8295454545454546\n",
      " Error Rate for Iris:  0.15909090909090906\n"
     ]
    }
   ],
   "source": [
    "# Using KNN classifier\n",
    "\"\"\" Acuuracy for Ionosphere Data :\"\"\"\n",
    "knn =  KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(I_X_train, I_y_train)\n",
    "print(\"Accuracy \",knn.score(I_X_test, I_y_test))\n",
    "\n",
    "\"\"\" Error Rate for KNN in 3\"\"\"\n",
    "error_rate_iono = (1-np.mean(Y_pred_Iono == I_y_test))\n",
    "print(\" Error Rate for Iris: \", error_rate_iono)      # Error rate is %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The above code will predict the labels by comparing the iris dataset with rows of test and rows of train by comparing each element by features column wise and by taking square root of them.\\nBy storing them in another array we take minimum value and predict it by comparing the labels.'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description of above code:\n",
    "\"\"\"The above code will predict the labels by comparing the iris dataset with rows of test and rows of train by comparing each element by features column wise and by taking square root of them.\n",
    "By storing them in another array we take minimum value and predict it by comparing the labels.\"\"\"\n",
    "\n",
    "# Test reults are given in out ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
